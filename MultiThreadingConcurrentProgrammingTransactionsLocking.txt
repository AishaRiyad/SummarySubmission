Multi-threading is the technique of running multiple threads within the same process so parts of a program can make progress “at the same time.” On multi-core CPUs, threads may execute in parallel; on a single core, the OS time-slices them so they appear concurrent. Threads share the same address space, which makes communication cheap (shared memory) but introduces risks: one thread’s writes can affect another thread’s reads. Good multi-threaded design minimizes shared mutable state, uses clear ownership, and leverages high-level constructs (thread pools, tasks, executors) rather than creating and managing raw threads directly.



Concurrent programming is the broader discipline of structuring programs that deal with multiple tasks overlapping in time. Concurrency doesn’t always require multiple threads or cores—event loops and async I/O are also concurrent. The core challenges are safety (no data races), liveness (no deadlocks or starvation), and performance (scalability with work and cores). You’ll often mix models: CPU-bound work might use threads; I/O-bound work might use async; coordination happens through queues, immutability, or synchronization. A useful mindset is to separate tasks (what needs doing) from execution (how it’s scheduled) and shared state (what must be consistent when tasks interact).


Transactions provide an all-or-nothing boundary for operations that must keep data consistent. The classic database guarantees are ACID: Atomicity (all changes happen or none), Consistency (rules/constraints remain true), Isolation (concurrent transactions don’t interfere in ways that break correctness), and Durability (committed data survives crashes). Isolation is where transactions intersect concurrency: different isolation levels (Read Uncommitted, Read Committed, Repeatable Read, Serializable) trade performance for correctness, influencing phenomena like dirty reads, non-repeatable reads, and phantom rows. In application code, you typically group a set of reads/writes into a transaction; the system ensures either a commit (visible to others) or a rollback (as if it never happened).


Transactions provide an all-or-nothing boundary for operations that must keep data consistent. The classic database guarantees are ACID: Atomicity (all changes happen or none), Consistency (rules/constraints remain true), Isolation (concurrent transactions don’t interfere in ways that break correctness), and Durability (committed data survives crashes). Isolation is where transactions intersect concurrency: different isolation levels (Read Uncommitted, Read Committed, Repeatable Read, Serializable) trade performance for correctness, influencing phenomena like dirty reads, non-repeatable reads, and phantom rows. In application code, you typically group a set of reads/writes into a transaction; the system ensures either a commit (visible to others) or a rollback (as if it never happened).


Locking is a fundamental mechanism to protect shared state under concurrency. A mutex (mutual exclusion lock) ensures only one thread modifies a resource at a time; read-write locks allow many readers or a single writer; reentrant locks let the same thread acquire the same lock multiple times safely. Over-locking harms scalability; under-locking risks races and corruption. The classic failure mode is deadlock, where threads wait on each other in a cycle. You reduce this risk by enforcing a global lock acquisition order, minimizing lock scope (hold time), and preferring higher-level abstractions (immutable data, message passing, transactional memory, STM). Optimistic strategies (like compare-and-swap or version checks) avoid holding locks up front: proceed, then verify no one else changed the state; if they did, retry. Databases use pessimistic or optimistic locking similarly at the row/page level to implement isolation.